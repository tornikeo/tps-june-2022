{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9860f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils as utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Tuple, List\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "dtype = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2945a5c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hf-internal-testing/tiny-random-distilbert were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'qa_outputs.weight', 'classifier.weight', 'pre_classifier.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'qa_outputs.bias', 'vocab_layer_norm.bias', 'classifier.bias', 'pre_classifier.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tok = AutoTokenizer.from_pretrained('hf-internal-testing/tiny-random-distilbert')\n",
    "model = AutoModel.from_pretrained('hf-internal-testing/tiny-random-distilbert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec529d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../input/tabular-playground-series-jun-2022/data.csv',\n",
    "                 index_col=0)\n",
    "mask = 1 - df.isna()\n",
    "df.fillna(0, inplace=True)\n",
    "# for col in df:\n",
    "#   if df[col].dtype == 'int64':\n",
    "#     df[col] = df[col].astype('int32')\n",
    "#   if df[col].dtype == 'float64':\n",
    "#     df[col] = df[col].astype('float32')\n",
    "df = df.astype('float32')\n",
    "mask = mask.astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70e1cf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "xdf = df.iloc[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "090361a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2499999999999956 % missing\n"
     ]
    }
   ],
   "source": [
    "print((1 - mask.mean().mean()) * 100, '% missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bed74c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of \n",
    "    -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "8eec62fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class MaskedTabularDataset(Dataset):\n",
    "  def __init__(self, df:pd.DataFrame, mask:pd.DataFrame, \n",
    "               mask_rate: float = .1,\n",
    "               validation=True\n",
    "              ):\n",
    "    self.data = torch.tensor(df.values, dtype=dtype)\n",
    "    self.mask = torch.tensor(mask.values, dtype=torch.uint8)\n",
    "    self.mask_rate = mask_rate\n",
    "    self.validation = validation\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "  \n",
    "  def __getitem__(self, indices) -> Tensor:\n",
    "    dslice = self.data[indices]\n",
    "    mslice = self.mask[indices]\n",
    "    \n",
    "    extra_mask = torch.rand(mslice.shape) > self.mask_rate\n",
    "    train_targets = (extra_mask == 0) & (mslice != 0)\n",
    "    train_mask = mslice * extra_mask\n",
    "    \n",
    "#     dslice = dslice.to(device)\n",
    "#     train_mask = train_mask.to(device)\n",
    "#     train_targets = train_targets.to(device)\n",
    "    \n",
    "    return dslice, train_mask, train_targets\n",
    "d = MaskedTabularDataset(df, mask, mask_rate=.1)\n",
    "x,y,z = d[:5]\n",
    "plt.imshow(y)\n",
    "plt.figure()\n",
    "plt.imshow(z)\n",
    "plt.figure()\n",
    "plt.imshow(mask.iloc[:5].values)\n",
    "assert x.shape == y.shape == x.shape == (5,80)\n",
    "assert ((x - df.iloc[:5].values) == 0).all()\n",
    "assert set(y.numpy().reshape(-1)) == {0., 1.}\n",
    "assert set(z.numpy().ravel()) == {0, 1}\n",
    "del d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "de400753",
   "metadata": {},
   "outputs": [],
   "source": [
    "NROWS = df.shape[0]\n",
    "NCOLS = df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "904a1743",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_NROWS = NROWS // 100\n",
    "TRAIN_NROWS = NROWS - VALID_NROWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f102162d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fullds = MaskedTabularDataset(df,mask, mask_rate=.1)\n",
    "trainds, valds = utils.data.random_split(fullds, \n",
    "                                         [TRAIN_NROWS, VALID_NROWS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4542e295",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2 ** 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6ae16b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindl = DataLoader(trainds, \n",
    "                     shuffle=True,\n",
    "                     num_workers=4,\n",
    "                     pin_memory=True,\n",
    "                     batch_size = BATCH_SIZE,)\n",
    "valdl = DataLoader(valds, \n",
    "                  shuffle=False,\n",
    "                  num_workers=4,\n",
    "                  pin_memory=True,\n",
    "                  batch_size = BATCH_SIZE * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b24a7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 420 ms, sys: 231 ms, total: 651 ms\n",
      "Wall time: 9.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i, batch in enumerate(traindl):\n",
    "  if i > 10:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "24eedb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "  for i, batch in enumerate(traindl):\n",
    "    data, train_mask, train_targets = batch\n",
    "    break\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "a4358824",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_embedding = nn.Linear(NCOLS, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2a64e11a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 40, 32])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = torch.randint(0, 1000, (32, 40))\n",
    "outp = model.embeddings(inp)\n",
    "outp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "05e1b722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.distilbert.modeling_distilbert.MultiHeadSelfAttention"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = torch.randn(32, 40, 32)\n",
    "# outp = model.transformer(inp)\n",
    "type(model.transformer.layer[0].attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "320c38ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "# type(model)\n",
    "transformers.models.distilbert.modeling_distilbert.DistilBertModel??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "4110e4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 1124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6d9341a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.F_1_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "787cf5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.histogram(x,bins)\n",
    "hist, bin_edges = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "bbc5a843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3611],\n",
       "        [ 0.8688],\n",
       "        [-1.7247],\n",
       "        [-0.8343],\n",
       "        [-1.8300],\n",
       "        [-0.7741],\n",
       "        [ 0.2882],\n",
       "        [ 0.6994],\n",
       "        [ 0.8065],\n",
       "        [ 1.7345],\n",
       "        [ 0.9767],\n",
       "        [-0.3214],\n",
       "        [ 0.3081],\n",
       "        [-0.1345],\n",
       "        [-0.9280],\n",
       "        [-1.9241],\n",
       "        [ 0.6305],\n",
       "        [-0.2968],\n",
       "        [ 0.4976],\n",
       "        [-0.3228],\n",
       "        [ 0.7203],\n",
       "        [-0.4218],\n",
       "        [ 1.1047],\n",
       "        [ 0.1777],\n",
       "        [ 0.2742],\n",
       "        [-0.4010],\n",
       "        [-2.0651],\n",
       "        [-0.7176],\n",
       "        [-0.2215],\n",
       "        [-0.4966],\n",
       "        [-0.1796],\n",
       "        [-0.2210],\n",
       "        [-2.7739],\n",
       "        [-1.1260],\n",
       "        [-0.0460],\n",
       "        [-0.5599],\n",
       "        [-0.1131],\n",
       "        [ 1.0634],\n",
       "        [-2.0862],\n",
       "        [-0.3766],\n",
       "        [ 0.8247],\n",
       "        [-1.5986]])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = torch.tensor([.9])\n",
    "a = torch.randn(42,1)\n",
    "a * c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "fac91c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4db93e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last hiddne state shape torch.Size([32, 40, 32])\n",
      "Number of tfm layers + 1 for embed 6\n",
      "torch.Size([32, 40, 32])\n",
      "torch.Size([32, 40, 32])\n",
      "Attention shape torch.Size([32, 4, 40, 40])\n",
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'hidden_states', 'attentions'])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input_ids = torch.randint(0, 1000, (32, 40))\n",
    "input_ids = torch.ones(32, 40).int()\n",
    "attention_mask = torch.randint(-100,100, (32, 40))\n",
    "outp= model(\n",
    "  input_ids=input_ids,\n",
    "  attention_mask=attention_mask,\n",
    "  input_embeds=\n",
    "  output_attentions = True,\n",
    "  output_hidden_states = True,)\n",
    "# [[k,v.shape,v.dtype] for k,v in outp.items()]\n",
    "print('Last hiddne state shape', outp.last_hidden_state.shape)\n",
    "print('Number of tfm layers + 1 for embed', len(outp.hidden_states)) # \n",
    "print(outp.hidden_states[0].shape)\n",
    "print(outp.hidden_states[1].shape)\n",
    "print('Attention shape', outp.attentions[0].shape)\n",
    "print(len(outp.attentions)) # Number of tfm layers\n",
    "outp.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a8ddaa76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (layer): ModuleList(\n",
       "    (0): TransformerBlock(\n",
       "      (attention): MultiHeadSelfAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (q_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (k_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (v_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (out_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (sa_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "      (ffn): FFN(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (lin1): Linear(in_features=32, out_features=37, bias=True)\n",
       "        (lin2): Linear(in_features=37, out_features=32, bias=True)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (output_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attention): MultiHeadSelfAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (q_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (k_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (v_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (out_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (sa_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "      (ffn): FFN(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (lin1): Linear(in_features=32, out_features=37, bias=True)\n",
       "        (lin2): Linear(in_features=37, out_features=32, bias=True)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (output_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attention): MultiHeadSelfAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (q_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (k_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (v_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (out_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (sa_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "      (ffn): FFN(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (lin1): Linear(in_features=32, out_features=37, bias=True)\n",
       "        (lin2): Linear(in_features=37, out_features=32, bias=True)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (output_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attention): MultiHeadSelfAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (q_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (k_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (v_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (out_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (sa_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "      (ffn): FFN(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (lin1): Linear(in_features=32, out_features=37, bias=True)\n",
       "        (lin2): Linear(in_features=37, out_features=32, bias=True)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (output_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attention): MultiHeadSelfAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (q_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (k_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (v_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (out_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (sa_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "      (ffn): FFN(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (lin1): Linear(in_features=32, out_features=37, bias=True)\n",
       "        (lin2): Linear(in_features=37, out_features=32, bias=True)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (output_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "bd7bfab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertConfig {\n",
       "  \"_name_or_path\": \"hf-internal-testing/tiny-random-distilbert\",\n",
       "  \"activation\": \"gelu\",\n",
       "  \"architectures\": [\n",
       "    \"DistilBertForSequenceClassification\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"dim\": 32,\n",
       "  \"dropout\": 0.1,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dim\": 37,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"distilbert\",\n",
       "  \"n_heads\": 4,\n",
       "  \"n_layers\": 5,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"qa_dropout\": 0.1,\n",
       "  \"seq_classif_dropout\": 0.2,\n",
       "  \"sinusoidal_pos_embds\": false,\n",
       "  \"transformers_version\": \"4.20.1\",\n",
       "  \"vocab_size\": 1124\n",
       "}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590a3a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outp.attentions[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2bf4fd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "# transformers.models.distilbert.modeling_distilbert.MultiHeadSelfAttention??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5140142",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, config: PretrainedConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_heads = config.n_heads\n",
    "        self.dim = config.dim\n",
    "        self.dropout = nn.Dropout(p=config.attention_dropout)\n",
    "\n",
    "        assert self.dim % self.n_heads == 0\n",
    "\n",
    "        self.q_lin = nn.Linear(in_features=config.dim, out_features=config.dim)\n",
    "        self.k_lin = nn.Linear(in_features=config.dim, out_features=config.dim)\n",
    "        self.v_lin = nn.Linear(in_features=config.dim, out_features=config.dim)\n",
    "        self.out_lin = nn.Linear(in_features=config.dim, out_features=config.dim)\n",
    "\n",
    "        self.pruned_heads: Set[int] = set()\n",
    "\n",
    "    def prune_heads(self, heads: List[int]):\n",
    "        attention_head_size = self.dim // self.n_heads\n",
    "        if len(heads) == 0:\n",
    "            return\n",
    "        heads, index = find_pruneable_heads_and_indices(heads, self.n_heads, attention_head_size, self.pruned_heads)\n",
    "        # Prune linear layers\n",
    "        self.q_lin = prune_linear_layer(self.q_lin, index)\n",
    "        self.k_lin = prune_linear_layer(self.k_lin, index)\n",
    "        self.v_lin = prune_linear_layer(self.v_lin, index)\n",
    "        self.out_lin = prune_linear_layer(self.out_lin, index, dim=1)\n",
    "        # Update hyper params\n",
    "        self.n_heads = self.n_heads - len(heads)\n",
    "        self.dim = attention_head_size * self.n_heads\n",
    "        self.pruned_heads = self.pruned_heads.union(heads)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "        mask: torch.Tensor,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "    ) -> Tuple[torch.Tensor, ...]:\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            query: torch.tensor(bs, seq_length, dim)\n",
    "            key: torch.tensor(bs, seq_length, dim)\n",
    "            value: torch.tensor(bs, seq_length, dim)\n",
    "            mask: torch.tensor(bs, seq_length)\n",
    "\n",
    "        Returns:\n",
    "            weights: torch.tensor(bs, n_heads, seq_length, seq_length) Attention weights context: torch.tensor(bs,\n",
    "            seq_length, dim) Contextualized layer. Optional: only if `output_attentions=True`\n",
    "        \"\"\"\n",
    "        bs, q_length, dim = query.size()\n",
    "        k_length = key.size(1)\n",
    "        # assert dim == self.dim, f'Dimensions do not match: {dim} input vs {self.dim} configured'\n",
    "        # assert key.size() == value.size()\n",
    "\n",
    "        dim_per_head = self.dim // self.n_heads\n",
    "\n",
    "        mask_reshp = (bs, 1, 1, k_length)\n",
    "\n",
    "        def shape(x: torch.Tensor) -> torch.Tensor:\n",
    "            \"\"\"separate heads\"\"\"\n",
    "            return x.view(bs, -1, self.n_heads, dim_per_head).transpose(1, 2)\n",
    "\n",
    "        def unshape(x: torch.Tensor) -> torch.Tensor:\n",
    "            \"\"\"group heads\"\"\"\n",
    "            return x.transpose(1, 2).contiguous().view(bs, -1, self.n_heads * dim_per_head)\n",
    "\n",
    "        q = shape(self.q_lin(query))  # (bs, n_heads, q_length, dim_per_head)\n",
    "        k = shape(self.k_lin(key))  # (bs, n_heads, k_length, dim_per_head)\n",
    "        v = shape(self.v_lin(value))  # (bs, n_heads, k_length, dim_per_head)\n",
    "\n",
    "        q = q / math.sqrt(dim_per_head)  # (bs, n_heads, q_length, dim_per_head)\n",
    "        scores = torch.matmul(q, k.transpose(2, 3))  # (bs, n_heads, q_length, k_length)\n",
    "        mask = (mask == 0).view(mask_reshp).expand_as(scores)  # (bs, n_heads, q_length, k_length)\n",
    "        scores = scores.masked_fill(mask, torch.tensor(-float(\"inf\")))  # (bs, n_heads, q_length, k_length)\n",
    "\n",
    "        weights = nn.functional.softmax(scores, dim=-1)  # (bs, n_heads, q_length, k_length)\n",
    "        weights = self.dropout(weights)  # (bs, n_heads, q_length, k_length)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            weights = weights * head_mask\n",
    "\n",
    "        context = torch.matmul(weights, v)  # (bs, n_heads, q_length, dim_per_head)\n",
    "        context = unshape(context)  # (bs, q_length, dim)\n",
    "        context = self.out_lin(context)  # (bs, q_length, dim)\n",
    "\n",
    "        if output_attentions:\n",
    "            return (context, weights)\n",
    "        else:\n",
    "            return (context,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d6250194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F_1_0</th>\n",
       "      <th>F_1_1</th>\n",
       "      <th>F_1_2</th>\n",
       "      <th>F_1_3</th>\n",
       "      <th>F_1_4</th>\n",
       "      <th>F_1_5</th>\n",
       "      <th>F_1_6</th>\n",
       "      <th>F_1_7</th>\n",
       "      <th>F_1_8</th>\n",
       "      <th>F_1_9</th>\n",
       "      <th>...</th>\n",
       "      <th>F_4_5</th>\n",
       "      <th>F_4_6</th>\n",
       "      <th>F_4_7</th>\n",
       "      <th>F_4_8</th>\n",
       "      <th>F_4_9</th>\n",
       "      <th>F_4_10</th>\n",
       "      <th>F_4_11</th>\n",
       "      <th>F_4_12</th>\n",
       "      <th>F_4_13</th>\n",
       "      <th>F_4_14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.013758</td>\n",
       "      <td>-0.003932</td>\n",
       "      <td>-0.001344</td>\n",
       "      <td>-0.004670</td>\n",
       "      <td>-0.006562</td>\n",
       "      <td>0.007233</td>\n",
       "      <td>0.012585</td>\n",
       "      <td>-0.067482</td>\n",
       "      <td>-0.001134</td>\n",
       "      <td>-0.016369</td>\n",
       "      <td>...</td>\n",
       "      <td>0.329664</td>\n",
       "      <td>-0.000261</td>\n",
       "      <td>0.341303</td>\n",
       "      <td>-0.075954</td>\n",
       "      <td>-0.093519</td>\n",
       "      <td>0.036682</td>\n",
       "      <td>0.481762</td>\n",
       "      <td>0.331615</td>\n",
       "      <td>0.356302</td>\n",
       "      <td>0.035026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.997102</td>\n",
       "      <td>0.988180</td>\n",
       "      <td>0.997567</td>\n",
       "      <td>0.998424</td>\n",
       "      <td>1.001309</td>\n",
       "      <td>0.986299</td>\n",
       "      <td>0.988003</td>\n",
       "      <td>0.725900</td>\n",
       "      <td>0.986922</td>\n",
       "      <td>0.990663</td>\n",
       "      <td>...</td>\n",
       "      <td>2.343665</td>\n",
       "      <td>2.245500</td>\n",
       "      <td>2.352445</td>\n",
       "      <td>0.779142</td>\n",
       "      <td>0.804810</td>\n",
       "      <td>0.702525</td>\n",
       "      <td>4.944890</td>\n",
       "      <td>2.355556</td>\n",
       "      <td>2.325799</td>\n",
       "      <td>0.762687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.950721</td>\n",
       "      <td>-3.711339</td>\n",
       "      <td>-3.991924</td>\n",
       "      <td>-3.638016</td>\n",
       "      <td>-3.694782</td>\n",
       "      <td>-3.905969</td>\n",
       "      <td>-4.565901</td>\n",
       "      <td>-5.089877</td>\n",
       "      <td>-3.626006</td>\n",
       "      <td>-3.723700</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.673384</td>\n",
       "      <td>-8.578391</td>\n",
       "      <td>-8.943967</td>\n",
       "      <td>-5.425965</td>\n",
       "      <td>-6.085913</td>\n",
       "      <td>-3.664145</td>\n",
       "      <td>-21.531364</td>\n",
       "      <td>-7.938445</td>\n",
       "      <td>-7.499790</td>\n",
       "      <td>-4.633961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.652578</td>\n",
       "      <td>-0.677542</td>\n",
       "      <td>-0.667383</td>\n",
       "      <td>-0.678866</td>\n",
       "      <td>-0.680896</td>\n",
       "      <td>-0.655918</td>\n",
       "      <td>-0.634941</td>\n",
       "      <td>-0.489951</td>\n",
       "      <td>-0.642562</td>\n",
       "      <td>-0.693838</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.258214</td>\n",
       "      <td>-1.515898</td>\n",
       "      <td>-1.187016</td>\n",
       "      <td>-0.509534</td>\n",
       "      <td>-0.571106</td>\n",
       "      <td>-0.374325</td>\n",
       "      <td>-2.826729</td>\n",
       "      <td>-1.265520</td>\n",
       "      <td>-1.234806</td>\n",
       "      <td>-0.389375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.002495</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.246422</td>\n",
       "      <td>-0.021871</td>\n",
       "      <td>0.336847</td>\n",
       "      <td>0.005902</td>\n",
       "      <td>-0.021037</td>\n",
       "      <td>0.082862</td>\n",
       "      <td>0.052698</td>\n",
       "      <td>0.306506</td>\n",
       "      <td>0.279058</td>\n",
       "      <td>0.113757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.682723</td>\n",
       "      <td>0.665338</td>\n",
       "      <td>0.673367</td>\n",
       "      <td>0.666814</td>\n",
       "      <td>0.662352</td>\n",
       "      <td>0.659837</td>\n",
       "      <td>0.668754</td>\n",
       "      <td>0.445423</td>\n",
       "      <td>0.649196</td>\n",
       "      <td>0.652642</td>\n",
       "      <td>...</td>\n",
       "      <td>1.878507</td>\n",
       "      <td>1.434053</td>\n",
       "      <td>1.896544</td>\n",
       "      <td>0.463040</td>\n",
       "      <td>0.457446</td>\n",
       "      <td>0.520567</td>\n",
       "      <td>3.473531</td>\n",
       "      <td>1.908401</td>\n",
       "      <td>1.922133</td>\n",
       "      <td>0.555714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.871386</td>\n",
       "      <td>3.582253</td>\n",
       "      <td>3.912949</td>\n",
       "      <td>3.595686</td>\n",
       "      <td>3.389098</td>\n",
       "      <td>3.536764</td>\n",
       "      <td>3.586039</td>\n",
       "      <td>1.885402</td>\n",
       "      <td>3.728087</td>\n",
       "      <td>4.757862</td>\n",
       "      <td>...</td>\n",
       "      <td>8.915140</td>\n",
       "      <td>8.586932</td>\n",
       "      <td>10.373207</td>\n",
       "      <td>2.282734</td>\n",
       "      <td>2.520878</td>\n",
       "      <td>2.195653</td>\n",
       "      <td>20.669884</td>\n",
       "      <td>8.588004</td>\n",
       "      <td>9.246682</td>\n",
       "      <td>2.212173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              F_1_0         F_1_1         F_1_2         F_1_3         F_1_4  \\\n",
       "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000   \n",
       "mean       0.013758     -0.003932     -0.001344     -0.004670     -0.006562   \n",
       "std        0.997102      0.988180      0.997567      0.998424      1.001309   \n",
       "min       -3.950721     -3.711339     -3.991924     -3.638016     -3.694782   \n",
       "25%       -0.652578     -0.677542     -0.667383     -0.678866     -0.680896   \n",
       "50%        0.000000      0.000000      0.000000     -0.002495      0.000000   \n",
       "75%        0.682723      0.665338      0.673367      0.666814      0.662352   \n",
       "max        3.871386      3.582253      3.912949      3.595686      3.389098   \n",
       "\n",
       "              F_1_5         F_1_6         F_1_7         F_1_8         F_1_9  \\\n",
       "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000   \n",
       "mean       0.007233      0.012585     -0.067482     -0.001134     -0.016369   \n",
       "std        0.986299      0.988003      0.725900      0.986922      0.990663   \n",
       "min       -3.905969     -4.565901     -5.089877     -3.626006     -3.723700   \n",
       "25%       -0.655918     -0.634941     -0.489951     -0.642562     -0.693838   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.659837      0.668754      0.445423      0.649196      0.652642   \n",
       "max        3.536764      3.586039      1.885402      3.728087      4.757862   \n",
       "\n",
       "       ...         F_4_5         F_4_6         F_4_7         F_4_8  \\\n",
       "count  ...  10000.000000  10000.000000  10000.000000  10000.000000   \n",
       "mean   ...      0.329664     -0.000261      0.341303     -0.075954   \n",
       "std    ...      2.343665      2.245500      2.352445      0.779142   \n",
       "min    ...     -8.673384     -8.578391     -8.943967     -5.425965   \n",
       "25%    ...     -1.258214     -1.515898     -1.187016     -0.509534   \n",
       "50%    ...      0.246422     -0.021871      0.336847      0.005902   \n",
       "75%    ...      1.878507      1.434053      1.896544      0.463040   \n",
       "max    ...      8.915140      8.586932     10.373207      2.282734   \n",
       "\n",
       "              F_4_9        F_4_10        F_4_11        F_4_12        F_4_13  \\\n",
       "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000   \n",
       "mean      -0.093519      0.036682      0.481762      0.331615      0.356302   \n",
       "std        0.804810      0.702525      4.944890      2.355556      2.325799   \n",
       "min       -6.085913     -3.664145    -21.531364     -7.938445     -7.499790   \n",
       "25%       -0.571106     -0.374325     -2.826729     -1.265520     -1.234806   \n",
       "50%       -0.021037      0.082862      0.052698      0.306506      0.279058   \n",
       "75%        0.457446      0.520567      3.473531      1.908401      1.922133   \n",
       "max        2.520878      2.195653     20.669884      8.588004      9.246682   \n",
       "\n",
       "             F_4_14  \n",
       "count  10000.000000  \n",
       "mean       0.035026  \n",
       "std        0.762687  \n",
       "min       -4.633961  \n",
       "25%       -0.389375  \n",
       "50%        0.113757  \n",
       "75%        0.555714  \n",
       "max        2.212173  \n",
       "\n",
       "[8 rows x 80 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10000).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111c7551",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1e5422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db99cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "  for i, batch in enumerate(data):\n",
    "    data, train_mask, train_targets = batch\n",
    "    # data shape [batch_size, num_cols, embed_dim]\n",
    "    # train_mask shape same as data, 0's and -infs\n",
    "    #   0's at train-set data points, -inf for other split data points\n",
    "    # train_targets shape same as data, 0's and 1's\n",
    "    #   1's at data points which should influence train loss, 0's otherwise\n",
    "    pred = model(data, train_mask)\n",
    "    loss = criterion(data, train_targets)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), .5)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96be13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "  data, targets = get_batch(train_data, i)\n",
    "  batch_size = data.size(0)\n",
    "  if batch_size != bptt:\n",
    "    src_mask = src_mask[:batch_size, :batch_size]\n",
    "  output = model(data, src_mask)\n",
    "  loss = criterion(output.view(-1, ntokens), targets)\n",
    "  optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "  optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0eb0c69b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(1124, 32, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 32)\n",
       "    (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (out_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=32, out_features=37, bias=True)\n",
       "          (lin2): Linear(in_features=37, out_features=32, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (1): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (out_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=32, out_features=37, bias=True)\n",
       "          (lin2): Linear(in_features=37, out_features=32, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (2): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (out_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=32, out_features=37, bias=True)\n",
       "          (lin2): Linear(in_features=37, out_features=32, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (3): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (out_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=32, out_features=37, bias=True)\n",
       "          (lin2): Linear(in_features=37, out_features=32, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (4): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (out_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=32, out_features=37, bias=True)\n",
       "          (lin2): Linear(in_features=37, out_features=32, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd478a60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "61c790cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/tornikeo/tab-transformer-pytorch\n",
      "  Cloning https://github.com/tornikeo/tab-transformer-pytorch to /tmp/pip-req-build-cq_qjc8g\n",
      "  Running command git clone -q https://github.com/tornikeo/tab-transformer-pytorch /tmp/pip-req-build-cq_qjc8g\n",
      "  Resolved https://github.com/tornikeo/tab-transformer-pytorch to commit 10cedc26ba54273fafe9ba2e15c47877a6d7b35f\n",
      "Collecting einops>=0.3\n",
      "  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: torch>=1.6 in /home/tornikeo/.miniconda3/envs/torch/lib/python3.8/site-packages (from tab-transformer-pytorch==0.1.4) (1.11.0)\n",
      "Requirement already satisfied: typing_extensions in /home/tornikeo/.miniconda3/envs/torch/lib/python3.8/site-packages (from torch>=1.6->tab-transformer-pytorch==0.1.4) (4.2.0)\n",
      "Building wheels for collected packages: tab-transformer-pytorch\n",
      "  Building wheel for tab-transformer-pytorch (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tab-transformer-pytorch: filename=tab_transformer_pytorch-0.1.4-py3-none-any.whl size=4579 sha256=6e59ce7fedd45a1c8f9ce22a63cbd71bc9d5b2391d713ec70278e0d01b589d81\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-bakx0yli/wheels/ea/d3/c1/b0003b87658c098a368fd6a003355be89412cbd2629d3985ec\n",
      "Successfully built tab-transformer-pytorch\n",
      "Installing collected packages: einops, tab-transformer-pytorch\n",
      "Successfully installed einops-0.4.1 tab-transformer-pytorch-0.1.4\n"
     ]
    }
   ],
   "source": [
    "# !pip install git+https://github.com/tornikeo/tab-transformer-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e0314528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tab_transformer_pytorch import TabTransformer\n",
    "\n",
    "cont_mean_std = torch.randn(10, 2)\n",
    "\n",
    "model = TabTransformer(\n",
    "    categories = (10, 5, 6, 5, 8),      # tuple containing the number of unique values within each category\n",
    "    num_continuous = 10,                # number of continuous values\n",
    "    dim = 32,                           # dimension, paper set at 32\n",
    "    dim_out = 1,                        # binary prediction, but could be anything\n",
    "    depth = 6,                          # depth, paper recommended 6\n",
    "    heads = 8,                          # heads, paper recommends 8\n",
    "    attn_dropout = 0.1,                 # post-attention dropout\n",
    "    ff_dropout = 0.1,                   # feed forward dropout\n",
    "    mlp_hidden_mults = (4, 2),          # relative multiples of each hidden dimension of the last mlp to logits\n",
    "    mlp_act = nn.ReLU(),                # activation for final mlp, defaults to relu, but could be anything else (selu etc)\n",
    "    continuous_mean_std = cont_mean_std # (optional) - normalize the continuous values before layer norm\n",
    ")\n",
    "\n",
    "x_categ = torch.randint(0, 5, (1, 5))     # category values, from 0 - max number of categories, in the order as passed into the constructor above\n",
    "x_cont = torch.randn(1, 10)               # assume continuous values are already normalized individually\n",
    "\n",
    "pred = model(x_categ, x_cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d65e8dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.ones(32,1,dtype=torch.int32) + torch.arange(10)\n",
    "outp = model(inp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd1fd599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10, 32])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outp.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "92d55ab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
