{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9860f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils as utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Tuple, List\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "dtype = torch.float32\n",
    "print('using', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2945a5c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hf-internal-testing/tiny-random-distilbert were not used when initializing DistilBertModel: ['pre_classifier.bias', 'qa_outputs.bias', 'pre_classifier.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'classifier.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'classifier.weight', 'qa_outputs.weight', 'vocab_transform.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tok = AutoTokenizer.from_pretrained('hf-internal-testing/tiny-random-distilbert')\n",
    "model = AutoModel.from_pretrained('hf-internal-testing/tiny-random-distilbert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec529d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../input/tabular-playground-series-jun-2022/data.csv',\n",
    "                 index_col=0)\n",
    "mask = 1 - df.isna()\n",
    "df.fillna(0, inplace=True)\n",
    "# for col in df:\n",
    "#   if df[col].dtype == 'int64':\n",
    "#     df[col] = df[col].astype('int32')\n",
    "#   if df[col].dtype == 'float64':\n",
    "#     df[col] = df[col].astype('float32')\n",
    "df = df.astype('float32')\n",
    "mask = mask.astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70e1cf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "xdf = df.iloc[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "090361a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2499999999999956 % missing\n"
     ]
    }
   ],
   "source": [
    "print((1 - mask.mean().mean()) * 100, '% missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bed74c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of \n",
    "    -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8eec62fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class MaskedTabularDataset(Dataset):\n",
    "  def __init__(self, df:pd.DataFrame, mask:pd.DataFrame, \n",
    "               mask_rate: float = .1,\n",
    "               validation=True\n",
    "              ):\n",
    "    self.data = torch.tensor(df.values, dtype=dtype)\n",
    "    self.mask = torch.tensor(mask.values, dtype=torch.uint8)\n",
    "    self.mask_rate = mask_rate\n",
    "    self.validation = validation\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "  \n",
    "  def __getitem__(self, indices) -> Tensor:\n",
    "    dslice = self.data[indices]\n",
    "    mslice = self.mask[indices]\n",
    "    \n",
    "    extra_mask = torch.rand(mslice.shape) > self.mask_rate\n",
    "    train_targets = (extra_mask == 0) & (mslice != 0)\n",
    "    train_mask = mslice * extra_mask\n",
    "    \n",
    "#     dslice = dslice.to(device)\n",
    "#     train_mask = train_mask.to(device)\n",
    "#     train_targets = train_targets.to(device)\n",
    "    \n",
    "    return dslice, train_mask, train_targets\n",
    "d = MaskedTabularDataset(df, mask, mask_rate=.1)\n",
    "x,y,z = d[:5]\n",
    "plt.imshow(y)\n",
    "plt.figure()\n",
    "plt.imshow(z)\n",
    "plt.figure()\n",
    "plt.imshow(mask.iloc[:5].values)\n",
    "assert x.shape == y.shape == x.shape == (5,80)\n",
    "assert ((x - df.iloc[:5].values) == 0).all()\n",
    "assert set(y.numpy().reshape(-1)) == {0., 1.}\n",
    "assert set(z.numpy().ravel()) == {0, 1}\n",
    "del d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de400753",
   "metadata": {},
   "outputs": [],
   "source": [
    "NROWS = df.shape[0]\n",
    "NCOLS = df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "904a1743",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_NROWS = NROWS // 100\n",
    "TRAIN_NROWS = NROWS - VALID_NROWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f102162d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fullds = MaskedTabularDataset(df,mask, mask_rate=.1)\n",
    "trainds, valds = utils.data.random_split(fullds, \n",
    "                                         [TRAIN_NROWS, VALID_NROWS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4542e295",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2 ** 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ae16b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindl = DataLoader(trainds, \n",
    "                     shuffle=True,\n",
    "                     num_workers=4,\n",
    "                     pin_memory=True,\n",
    "                     batch_size = BATCH_SIZE,)\n",
    "valdl = DataLoader(valds, \n",
    "                  shuffle=False,\n",
    "                  num_workers=4,\n",
    "                  pin_memory=True,\n",
    "                  batch_size = BATCH_SIZE * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b24a7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# for i, batch in enumerate(traindl):\n",
    "#   if i > 10:\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24eedb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "  for i, batch in enumerate(traindl):\n",
    "    data, train_mask, train_targets = batch\n",
    "    break\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4358824",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_embedding = nn.Linear(NCOLS, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a64e11a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 40, 32])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = torch.randint(0, 1000, (32, 40))\n",
    "outp = model.embeddings(inp)\n",
    "outp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05e1b722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.distilbert.modeling_distilbert.MultiHeadSelfAttention"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = torch.randn(32, 40, 32)\n",
    "# outp = model.transformer(inp)\n",
    "type(model.transformer.layer[0].attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "320c38ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformers\n",
    "# # type(model)\n",
    "# transformers.models.distilbert.modeling_distilbert.DistilBertModel??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4110e4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 1124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d9341a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.F_1_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "787cf5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.histogram(x,bins)\n",
    "hist, bin_edges = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bbc5a843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3680],\n",
       "        [-0.5996],\n",
       "        [-0.4485],\n",
       "        [-0.2061],\n",
       "        [ 0.0133],\n",
       "        [-1.1914],\n",
       "        [ 0.9577],\n",
       "        [ 0.3574],\n",
       "        [-1.1935],\n",
       "        [ 1.1373],\n",
       "        [ 1.0663],\n",
       "        [-1.2226],\n",
       "        [ 0.3369],\n",
       "        [ 0.2016],\n",
       "        [ 0.6633],\n",
       "        [ 0.6175],\n",
       "        [ 0.2657],\n",
       "        [ 2.1318],\n",
       "        [-0.5704],\n",
       "        [-0.4019],\n",
       "        [ 0.0274],\n",
       "        [ 0.1663],\n",
       "        [ 0.1969],\n",
       "        [ 0.6465],\n",
       "        [ 0.6914],\n",
       "        [ 0.3307],\n",
       "        [-0.2495],\n",
       "        [-0.3798],\n",
       "        [ 1.7913],\n",
       "        [-2.4259],\n",
       "        [-0.4629],\n",
       "        [ 0.6307],\n",
       "        [ 1.2793],\n",
       "        [-0.8710],\n",
       "        [-2.5565],\n",
       "        [ 0.9516],\n",
       "        [ 0.2915],\n",
       "        [ 1.0693],\n",
       "        [-0.9620],\n",
       "        [ 0.1351],\n",
       "        [ 0.6245],\n",
       "        [ 0.4525]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = torch.tensor([.9])\n",
    "a = torch.randn(42,1)\n",
    "a * c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "fac91c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4db93e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last hiddne state shape torch.Size([32, 40, 32])\n",
      "Number of tfm layers + 1 for embed 6\n",
      "torch.Size([32, 40, 32])\n",
      "torch.Size([32, 40, 32])\n",
      "Attention shape torch.Size([32, 4, 40, 40])\n",
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'hidden_states', 'attentions'])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input_ids = torch.randint(0, 1000, (32, 40))\n",
    "input_ids = torch.ones(32, 40).int()\n",
    "attention_mask = torch.randint(-100,100, (32, 40))\n",
    "outp= model(\n",
    "  input_ids=input_ids,\n",
    "  attention_mask=attention_mask,\n",
    "  input_embeds=\n",
    "  output_attentions = True,\n",
    "  output_hidden_states = True,)\n",
    "# [[k,v.shape,v.dtype] for k,v in outp.items()]\n",
    "print('Last hiddne state shape', outp.last_hidden_state.shape)\n",
    "print('Number of tfm layers + 1 for embed', len(outp.hidden_states)) # \n",
    "print(outp.hidden_states[0].shape)\n",
    "print(outp.hidden_states[1].shape)\n",
    "print('Attention shape', outp.attentions[0].shape)\n",
    "print(len(outp.attentions)) # Number of tfm layers\n",
    "outp.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a8ddaa76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (layer): ModuleList(\n",
       "    (0): TransformerBlock(\n",
       "      (attention): MultiHeadSelfAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (q_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (k_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (v_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (out_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (sa_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "      (ffn): FFN(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (lin1): Linear(in_features=32, out_features=37, bias=True)\n",
       "        (lin2): Linear(in_features=37, out_features=32, bias=True)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (output_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attention): MultiHeadSelfAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (q_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (k_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (v_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (out_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (sa_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "      (ffn): FFN(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (lin1): Linear(in_features=32, out_features=37, bias=True)\n",
       "        (lin2): Linear(in_features=37, out_features=32, bias=True)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (output_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attention): MultiHeadSelfAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (q_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (k_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (v_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (out_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (sa_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "      (ffn): FFN(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (lin1): Linear(in_features=32, out_features=37, bias=True)\n",
       "        (lin2): Linear(in_features=37, out_features=32, bias=True)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (output_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attention): MultiHeadSelfAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (q_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (k_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (v_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (out_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (sa_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "      (ffn): FFN(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (lin1): Linear(in_features=32, out_features=37, bias=True)\n",
       "        (lin2): Linear(in_features=37, out_features=32, bias=True)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (output_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attention): MultiHeadSelfAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (q_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (k_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (v_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (out_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (sa_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "      (ffn): FFN(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (lin1): Linear(in_features=32, out_features=37, bias=True)\n",
       "        (lin2): Linear(in_features=37, out_features=32, bias=True)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (output_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "bd7bfab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertConfig {\n",
       "  \"_name_or_path\": \"hf-internal-testing/tiny-random-distilbert\",\n",
       "  \"activation\": \"gelu\",\n",
       "  \"architectures\": [\n",
       "    \"DistilBertForSequenceClassification\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"dim\": 32,\n",
       "  \"dropout\": 0.1,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dim\": 37,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"distilbert\",\n",
       "  \"n_heads\": 4,\n",
       "  \"n_layers\": 5,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"qa_dropout\": 0.1,\n",
       "  \"seq_classif_dropout\": 0.2,\n",
       "  \"sinusoidal_pos_embds\": false,\n",
       "  \"transformers_version\": \"4.20.1\",\n",
       "  \"vocab_size\": 1124\n",
       "}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590a3a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outp.attentions[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2bf4fd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "# transformers.models.distilbert.modeling_distilbert.MultiHeadSelfAttention??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5140142",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, config: PretrainedConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_heads = config.n_heads\n",
    "        self.dim = config.dim\n",
    "        self.dropout = nn.Dropout(p=config.attention_dropout)\n",
    "\n",
    "        assert self.dim % self.n_heads == 0\n",
    "\n",
    "        self.q_lin = nn.Linear(in_features=config.dim, out_features=config.dim)\n",
    "        self.k_lin = nn.Linear(in_features=config.dim, out_features=config.dim)\n",
    "        self.v_lin = nn.Linear(in_features=config.dim, out_features=config.dim)\n",
    "        self.out_lin = nn.Linear(in_features=config.dim, out_features=config.dim)\n",
    "\n",
    "        self.pruned_heads: Set[int] = set()\n",
    "\n",
    "    def prune_heads(self, heads: List[int]):\n",
    "        attention_head_size = self.dim // self.n_heads\n",
    "        if len(heads) == 0:\n",
    "            return\n",
    "        heads, index = find_pruneable_heads_and_indices(heads, self.n_heads, attention_head_size, self.pruned_heads)\n",
    "        # Prune linear layers\n",
    "        self.q_lin = prune_linear_layer(self.q_lin, index)\n",
    "        self.k_lin = prune_linear_layer(self.k_lin, index)\n",
    "        self.v_lin = prune_linear_layer(self.v_lin, index)\n",
    "        self.out_lin = prune_linear_layer(self.out_lin, index, dim=1)\n",
    "        # Update hyper params\n",
    "        self.n_heads = self.n_heads - len(heads)\n",
    "        self.dim = attention_head_size * self.n_heads\n",
    "        self.pruned_heads = self.pruned_heads.union(heads)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "        mask: torch.Tensor,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "    ) -> Tuple[torch.Tensor, ...]:\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            query: torch.tensor(bs, seq_length, dim)\n",
    "            key: torch.tensor(bs, seq_length, dim)\n",
    "            value: torch.tensor(bs, seq_length, dim)\n",
    "            mask: torch.tensor(bs, seq_length)\n",
    "\n",
    "        Returns:\n",
    "            weights: torch.tensor(bs, n_heads, seq_length, seq_length) Attention weights context: torch.tensor(bs,\n",
    "            seq_length, dim) Contextualized layer. Optional: only if `output_attentions=True`\n",
    "        \"\"\"\n",
    "        bs, q_length, dim = query.size()\n",
    "        k_length = key.size(1)\n",
    "        # assert dim == self.dim, f'Dimensions do not match: {dim} input vs {self.dim} configured'\n",
    "        # assert key.size() == value.size()\n",
    "\n",
    "        dim_per_head = self.dim // self.n_heads\n",
    "\n",
    "        mask_reshp = (bs, 1, 1, k_length)\n",
    "\n",
    "        def shape(x: torch.Tensor) -> torch.Tensor:\n",
    "            \"\"\"separate heads\"\"\"\n",
    "            return x.view(bs, -1, self.n_heads, dim_per_head).transpose(1, 2)\n",
    "\n",
    "        def unshape(x: torch.Tensor) -> torch.Tensor:\n",
    "            \"\"\"group heads\"\"\"\n",
    "            return x.transpose(1, 2).contiguous().view(bs, -1, self.n_heads * dim_per_head)\n",
    "\n",
    "        q = shape(self.q_lin(query))  # (bs, n_heads, q_length, dim_per_head)\n",
    "        k = shape(self.k_lin(key))  # (bs, n_heads, k_length, dim_per_head)\n",
    "        v = shape(self.v_lin(value))  # (bs, n_heads, k_length, dim_per_head)\n",
    "\n",
    "        q = q / math.sqrt(dim_per_head)  # (bs, n_heads, q_length, dim_per_head)\n",
    "        scores = torch.matmul(q, k.transpose(2, 3))  # (bs, n_heads, q_length, k_length)\n",
    "        mask = (mask == 0).view(mask_reshp).expand_as(scores)  # (bs, n_heads, q_length, k_length)\n",
    "        scores = scores.masked_fill(mask, torch.tensor(-float(\"inf\")))  # (bs, n_heads, q_length, k_length)\n",
    "\n",
    "        weights = nn.functional.softmax(scores, dim=-1)  # (bs, n_heads, q_length, k_length)\n",
    "        weights = self.dropout(weights)  # (bs, n_heads, q_length, k_length)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            weights = weights * head_mask\n",
    "\n",
    "        context = torch.matmul(weights, v)  # (bs, n_heads, q_length, dim_per_head)\n",
    "        context = unshape(context)  # (bs, q_length, dim)\n",
    "        context = self.out_lin(context)  # (bs, q_length, dim)\n",
    "\n",
    "        if output_attentions:\n",
    "            return (context, weights)\n",
    "        else:\n",
    "            return (context,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d6250194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F_1_0</th>\n",
       "      <th>F_1_1</th>\n",
       "      <th>F_1_2</th>\n",
       "      <th>F_1_3</th>\n",
       "      <th>F_1_4</th>\n",
       "      <th>F_1_5</th>\n",
       "      <th>F_1_6</th>\n",
       "      <th>F_1_7</th>\n",
       "      <th>F_1_8</th>\n",
       "      <th>F_1_9</th>\n",
       "      <th>...</th>\n",
       "      <th>F_4_5</th>\n",
       "      <th>F_4_6</th>\n",
       "      <th>F_4_7</th>\n",
       "      <th>F_4_8</th>\n",
       "      <th>F_4_9</th>\n",
       "      <th>F_4_10</th>\n",
       "      <th>F_4_11</th>\n",
       "      <th>F_4_12</th>\n",
       "      <th>F_4_13</th>\n",
       "      <th>F_4_14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.013758</td>\n",
       "      <td>-0.003932</td>\n",
       "      <td>-0.001344</td>\n",
       "      <td>-0.004670</td>\n",
       "      <td>-0.006562</td>\n",
       "      <td>0.007233</td>\n",
       "      <td>0.012585</td>\n",
       "      <td>-0.067482</td>\n",
       "      <td>-0.001134</td>\n",
       "      <td>-0.016369</td>\n",
       "      <td>...</td>\n",
       "      <td>0.329664</td>\n",
       "      <td>-0.000261</td>\n",
       "      <td>0.341303</td>\n",
       "      <td>-0.075954</td>\n",
       "      <td>-0.093519</td>\n",
       "      <td>0.036682</td>\n",
       "      <td>0.481762</td>\n",
       "      <td>0.331615</td>\n",
       "      <td>0.356302</td>\n",
       "      <td>0.035026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.997102</td>\n",
       "      <td>0.988180</td>\n",
       "      <td>0.997567</td>\n",
       "      <td>0.998424</td>\n",
       "      <td>1.001309</td>\n",
       "      <td>0.986299</td>\n",
       "      <td>0.988003</td>\n",
       "      <td>0.725900</td>\n",
       "      <td>0.986922</td>\n",
       "      <td>0.990663</td>\n",
       "      <td>...</td>\n",
       "      <td>2.343665</td>\n",
       "      <td>2.245500</td>\n",
       "      <td>2.352445</td>\n",
       "      <td>0.779142</td>\n",
       "      <td>0.804810</td>\n",
       "      <td>0.702525</td>\n",
       "      <td>4.944890</td>\n",
       "      <td>2.355556</td>\n",
       "      <td>2.325799</td>\n",
       "      <td>0.762687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.950721</td>\n",
       "      <td>-3.711339</td>\n",
       "      <td>-3.991924</td>\n",
       "      <td>-3.638016</td>\n",
       "      <td>-3.694782</td>\n",
       "      <td>-3.905969</td>\n",
       "      <td>-4.565901</td>\n",
       "      <td>-5.089877</td>\n",
       "      <td>-3.626006</td>\n",
       "      <td>-3.723700</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.673384</td>\n",
       "      <td>-8.578391</td>\n",
       "      <td>-8.943967</td>\n",
       "      <td>-5.425965</td>\n",
       "      <td>-6.085913</td>\n",
       "      <td>-3.664145</td>\n",
       "      <td>-21.531364</td>\n",
       "      <td>-7.938445</td>\n",
       "      <td>-7.499790</td>\n",
       "      <td>-4.633961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.652578</td>\n",
       "      <td>-0.677542</td>\n",
       "      <td>-0.667383</td>\n",
       "      <td>-0.678866</td>\n",
       "      <td>-0.680896</td>\n",
       "      <td>-0.655918</td>\n",
       "      <td>-0.634941</td>\n",
       "      <td>-0.489951</td>\n",
       "      <td>-0.642562</td>\n",
       "      <td>-0.693838</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.258214</td>\n",
       "      <td>-1.515898</td>\n",
       "      <td>-1.187016</td>\n",
       "      <td>-0.509534</td>\n",
       "      <td>-0.571106</td>\n",
       "      <td>-0.374325</td>\n",
       "      <td>-2.826729</td>\n",
       "      <td>-1.265520</td>\n",
       "      <td>-1.234806</td>\n",
       "      <td>-0.389375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.002495</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.246422</td>\n",
       "      <td>-0.021871</td>\n",
       "      <td>0.336847</td>\n",
       "      <td>0.005902</td>\n",
       "      <td>-0.021037</td>\n",
       "      <td>0.082862</td>\n",
       "      <td>0.052698</td>\n",
       "      <td>0.306506</td>\n",
       "      <td>0.279058</td>\n",
       "      <td>0.113757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.682723</td>\n",
       "      <td>0.665338</td>\n",
       "      <td>0.673367</td>\n",
       "      <td>0.666814</td>\n",
       "      <td>0.662352</td>\n",
       "      <td>0.659837</td>\n",
       "      <td>0.668754</td>\n",
       "      <td>0.445423</td>\n",
       "      <td>0.649196</td>\n",
       "      <td>0.652642</td>\n",
       "      <td>...</td>\n",
       "      <td>1.878507</td>\n",
       "      <td>1.434053</td>\n",
       "      <td>1.896544</td>\n",
       "      <td>0.463040</td>\n",
       "      <td>0.457446</td>\n",
       "      <td>0.520567</td>\n",
       "      <td>3.473531</td>\n",
       "      <td>1.908401</td>\n",
       "      <td>1.922133</td>\n",
       "      <td>0.555714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.871386</td>\n",
       "      <td>3.582253</td>\n",
       "      <td>3.912949</td>\n",
       "      <td>3.595686</td>\n",
       "      <td>3.389098</td>\n",
       "      <td>3.536764</td>\n",
       "      <td>3.586039</td>\n",
       "      <td>1.885402</td>\n",
       "      <td>3.728087</td>\n",
       "      <td>4.757862</td>\n",
       "      <td>...</td>\n",
       "      <td>8.915140</td>\n",
       "      <td>8.586932</td>\n",
       "      <td>10.373207</td>\n",
       "      <td>2.282734</td>\n",
       "      <td>2.520878</td>\n",
       "      <td>2.195653</td>\n",
       "      <td>20.669884</td>\n",
       "      <td>8.588004</td>\n",
       "      <td>9.246682</td>\n",
       "      <td>2.212173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              F_1_0         F_1_1         F_1_2         F_1_3         F_1_4  \\\n",
       "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000   \n",
       "mean       0.013758     -0.003932     -0.001344     -0.004670     -0.006562   \n",
       "std        0.997102      0.988180      0.997567      0.998424      1.001309   \n",
       "min       -3.950721     -3.711339     -3.991924     -3.638016     -3.694782   \n",
       "25%       -0.652578     -0.677542     -0.667383     -0.678866     -0.680896   \n",
       "50%        0.000000      0.000000      0.000000     -0.002495      0.000000   \n",
       "75%        0.682723      0.665338      0.673367      0.666814      0.662352   \n",
       "max        3.871386      3.582253      3.912949      3.595686      3.389098   \n",
       "\n",
       "              F_1_5         F_1_6         F_1_7         F_1_8         F_1_9  \\\n",
       "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000   \n",
       "mean       0.007233      0.012585     -0.067482     -0.001134     -0.016369   \n",
       "std        0.986299      0.988003      0.725900      0.986922      0.990663   \n",
       "min       -3.905969     -4.565901     -5.089877     -3.626006     -3.723700   \n",
       "25%       -0.655918     -0.634941     -0.489951     -0.642562     -0.693838   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.659837      0.668754      0.445423      0.649196      0.652642   \n",
       "max        3.536764      3.586039      1.885402      3.728087      4.757862   \n",
       "\n",
       "       ...         F_4_5         F_4_6         F_4_7         F_4_8  \\\n",
       "count  ...  10000.000000  10000.000000  10000.000000  10000.000000   \n",
       "mean   ...      0.329664     -0.000261      0.341303     -0.075954   \n",
       "std    ...      2.343665      2.245500      2.352445      0.779142   \n",
       "min    ...     -8.673384     -8.578391     -8.943967     -5.425965   \n",
       "25%    ...     -1.258214     -1.515898     -1.187016     -0.509534   \n",
       "50%    ...      0.246422     -0.021871      0.336847      0.005902   \n",
       "75%    ...      1.878507      1.434053      1.896544      0.463040   \n",
       "max    ...      8.915140      8.586932     10.373207      2.282734   \n",
       "\n",
       "              F_4_9        F_4_10        F_4_11        F_4_12        F_4_13  \\\n",
       "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000   \n",
       "mean      -0.093519      0.036682      0.481762      0.331615      0.356302   \n",
       "std        0.804810      0.702525      4.944890      2.355556      2.325799   \n",
       "min       -6.085913     -3.664145    -21.531364     -7.938445     -7.499790   \n",
       "25%       -0.571106     -0.374325     -2.826729     -1.265520     -1.234806   \n",
       "50%       -0.021037      0.082862      0.052698      0.306506      0.279058   \n",
       "75%        0.457446      0.520567      3.473531      1.908401      1.922133   \n",
       "max        2.520878      2.195653     20.669884      8.588004      9.246682   \n",
       "\n",
       "             F_4_14  \n",
       "count  10000.000000  \n",
       "mean       0.035026  \n",
       "std        0.762687  \n",
       "min       -4.633961  \n",
       "25%       -0.389375  \n",
       "50%        0.113757  \n",
       "75%        0.555714  \n",
       "max        2.212173  \n",
       "\n",
       "[8 rows x 80 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10000).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111c7551",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1e5422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db99cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "  for i, batch in enumerate(data):\n",
    "    data, train_mask, train_targets = batch\n",
    "    # data shape [batch_size, num_cols, embed_dim]\n",
    "    # train_mask shape same as data, 0's and -infs\n",
    "    #   0's at train-set data points, -inf for other split data points\n",
    "    # train_targets shape same as data, 0's and 1's\n",
    "    #   1's at data points which should influence train loss, 0's otherwise\n",
    "    pred = model(data, train_mask)\n",
    "    loss = criterion(data, train_targets)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), .5)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96be13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "  data, targets = get_batch(train_data, i)\n",
    "  batch_size = data.size(0)\n",
    "  if batch_size != bptt:\n",
    "    src_mask = src_mask[:batch_size, :batch_size]\n",
    "  output = model(data, src_mask)\n",
    "  loss = criterion(output.view(-1, ntokens), targets)\n",
    "  optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "  optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0eb0c69b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(1124, 32, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 32)\n",
       "    (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (out_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=32, out_features=37, bias=True)\n",
       "          (lin2): Linear(in_features=37, out_features=32, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (1): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (out_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=32, out_features=37, bias=True)\n",
       "          (lin2): Linear(in_features=37, out_features=32, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (2): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (out_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=32, out_features=37, bias=True)\n",
       "          (lin2): Linear(in_features=37, out_features=32, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (3): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (out_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=32, out_features=37, bias=True)\n",
       "          (lin2): Linear(in_features=37, out_features=32, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (4): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (out_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=32, out_features=37, bias=True)\n",
       "          (lin2): Linear(in_features=37, out_features=32, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd478a60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "61c790cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/tornikeo/tab-transformer-pytorch\n",
      "  Cloning https://github.com/tornikeo/tab-transformer-pytorch to /tmp/pip-req-build-cq_qjc8g\n",
      "  Running command git clone -q https://github.com/tornikeo/tab-transformer-pytorch /tmp/pip-req-build-cq_qjc8g\n",
      "  Resolved https://github.com/tornikeo/tab-transformer-pytorch to commit 10cedc26ba54273fafe9ba2e15c47877a6d7b35f\n",
      "Collecting einops>=0.3\n",
      "  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: torch>=1.6 in /home/tornikeo/.miniconda3/envs/torch/lib/python3.8/site-packages (from tab-transformer-pytorch==0.1.4) (1.11.0)\n",
      "Requirement already satisfied: typing_extensions in /home/tornikeo/.miniconda3/envs/torch/lib/python3.8/site-packages (from torch>=1.6->tab-transformer-pytorch==0.1.4) (4.2.0)\n",
      "Building wheels for collected packages: tab-transformer-pytorch\n",
      "  Building wheel for tab-transformer-pytorch (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tab-transformer-pytorch: filename=tab_transformer_pytorch-0.1.4-py3-none-any.whl size=4579 sha256=6e59ce7fedd45a1c8f9ce22a63cbd71bc9d5b2391d713ec70278e0d01b589d81\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-bakx0yli/wheels/ea/d3/c1/b0003b87658c098a368fd6a003355be89412cbd2629d3985ec\n",
      "Successfully built tab-transformer-pytorch\n",
      "Installing collected packages: einops, tab-transformer-pytorch\n",
      "Successfully installed einops-0.4.1 tab-transformer-pytorch-0.1.4\n"
     ]
    }
   ],
   "source": [
    "# !pip install git+https://github.com/tornikeo/tab-transformer-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e0314528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tab_transformer_pytorch import TabTransformer\n",
    "\n",
    "cont_mean_std = torch.randn(10, 2)\n",
    "\n",
    "model = TabTransformer(\n",
    "    categories = (10, 5, 6, 5, 8),      # tuple containing the number of unique values within each category\n",
    "    num_continuous = 10,                # number of continuous values\n",
    "    dim = 32,                           # dimension, paper set at 32\n",
    "    dim_out = 1,                        # binary prediction, but could be anything\n",
    "    depth = 6,                          # depth, paper recommended 6\n",
    "    heads = 8,                          # heads, paper recommends 8\n",
    "    attn_dropout = 0.1,                 # post-attention dropout\n",
    "    ff_dropout = 0.1,                   # feed forward dropout\n",
    "    mlp_hidden_mults = (4, 2),          # relative multiples of each hidden dimension of the last mlp to logits\n",
    "    mlp_act = nn.ReLU(),                # activation for final mlp, defaults to relu, but could be anything else (selu etc)\n",
    "    continuous_mean_std = cont_mean_std # (optional) - normalize the continuous values before layer norm\n",
    ")\n",
    "\n",
    "x_categ = torch.randint(0, 5, (1, 5))     # category values, from 0 - max number of categories, in the order as passed into the constructor above\n",
    "x_cont = torch.randn(1, 10)               # assume continuous values are already normalized individually\n",
    "\n",
    "pred = model(x_categ, x_cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d65e8dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.ones(32,1,dtype=torch.int32) + torch.arange(10)\n",
    "outp = model(inp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd1fd599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10, 32])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outp.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "92d55ab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
