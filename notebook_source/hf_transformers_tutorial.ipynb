{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9860f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Tuple, List\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "dtype = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2945a5c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hf-internal-testing/tiny-random-distilbert were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'classifier.weight', 'pre_classifier.bias', 'vocab_projector.bias', 'qa_outputs.weight', 'vocab_transform.weight', 'classifier.bias', 'qa_outputs.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'pre_classifier.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tok = AutoTokenizer.from_pretrained('hf-internal-testing/tiny-random-distilbert')\n",
    "model = AutoModel.from_pretrained('hf-internal-testing/tiny-random-distilbert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ae0a8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../input/tabular-playground-series-jun-2022/data.csv',\n",
    "                 index_col=0)\n",
    "mask = 1 - df.isna()\n",
    "\n",
    "for col in df:\n",
    "  if df[col].dtype == 'int64':\n",
    "    df[col] = df[col].astype('int32')\n",
    "  if df[col].dtype == 'float64':\n",
    "    df[col] = df[col].astype('float32')\n",
    "mask = mask.astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "431a0fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36ad54ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 0,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 0, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 0, 1]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(mask.values[:100,:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a1cdcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedTabularDataset(Dataset):\n",
    "  def __init__(self, df:pd.DataFrame, mask:pd.DataFrame):\n",
    "    self.df = df\n",
    "    self.mask = torchmask\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.df)\n",
    "  \n",
    "  def __getitem__(self, indices: Tuple[int]) -> Tensor:\n",
    "    return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f07fc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "  for i, batch in enumerate(data):\n",
    "    data, train_mask, train_targets = batch\n",
    "    # data shape [batch_size, num_cols, embed_dim]\n",
    "    # train_mask shape same as data, 0's and -infs\n",
    "    #   0's at train-set data points, -inf for other split data points\n",
    "    # train_targets shape same as data, 0's and 1's\n",
    "    #   1's at data points which should influence train loss, 0's otherwise\n",
    "    pred = model(data, train_mask)\n",
    "    loss = criterion(data, train_targets)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), .5)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d627e484",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "  data, targets = get_batch(train_data, i)\n",
    "  batch_size = data.size(0)\n",
    "  if batch_size != bptt:\n",
    "    src_mask = src_mask[:batch_size, :batch_size]\n",
    "  output = model(data, src_mask)\n",
    "  loss = criterion(output.view(-1, ntokens), targets)\n",
    "  optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "  optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0eb0c69b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(1124, 32, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 32)\n",
       "    (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (out_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=32, out_features=37, bias=True)\n",
       "          (lin2): Linear(in_features=37, out_features=32, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (1): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (out_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=32, out_features=37, bias=True)\n",
       "          (lin2): Linear(in_features=37, out_features=32, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (2): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (out_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=32, out_features=37, bias=True)\n",
       "          (lin2): Linear(in_features=37, out_features=32, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (3): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (out_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=32, out_features=37, bias=True)\n",
       "          (lin2): Linear(in_features=37, out_features=32, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (4): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (out_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=32, out_features=37, bias=True)\n",
       "          (lin2): Linear(in_features=37, out_features=32, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e529d37f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "61c790cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/tornikeo/tab-transformer-pytorch\n",
      "  Cloning https://github.com/tornikeo/tab-transformer-pytorch to /tmp/pip-req-build-cq_qjc8g\n",
      "  Running command git clone -q https://github.com/tornikeo/tab-transformer-pytorch /tmp/pip-req-build-cq_qjc8g\n",
      "  Resolved https://github.com/tornikeo/tab-transformer-pytorch to commit 10cedc26ba54273fafe9ba2e15c47877a6d7b35f\n",
      "Collecting einops>=0.3\n",
      "  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: torch>=1.6 in /home/tornikeo/.miniconda3/envs/torch/lib/python3.8/site-packages (from tab-transformer-pytorch==0.1.4) (1.11.0)\n",
      "Requirement already satisfied: typing_extensions in /home/tornikeo/.miniconda3/envs/torch/lib/python3.8/site-packages (from torch>=1.6->tab-transformer-pytorch==0.1.4) (4.2.0)\n",
      "Building wheels for collected packages: tab-transformer-pytorch\n",
      "  Building wheel for tab-transformer-pytorch (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tab-transformer-pytorch: filename=tab_transformer_pytorch-0.1.4-py3-none-any.whl size=4579 sha256=6e59ce7fedd45a1c8f9ce22a63cbd71bc9d5b2391d713ec70278e0d01b589d81\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-bakx0yli/wheels/ea/d3/c1/b0003b87658c098a368fd6a003355be89412cbd2629d3985ec\n",
      "Successfully built tab-transformer-pytorch\n",
      "Installing collected packages: einops, tab-transformer-pytorch\n",
      "Successfully installed einops-0.4.1 tab-transformer-pytorch-0.1.4\n"
     ]
    }
   ],
   "source": [
    "# !pip install git+https://github.com/tornikeo/tab-transformer-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e0314528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tab_transformer_pytorch import TabTransformer\n",
    "\n",
    "cont_mean_std = torch.randn(10, 2)\n",
    "\n",
    "model = TabTransformer(\n",
    "    categories = (10, 5, 6, 5, 8),      # tuple containing the number of unique values within each category\n",
    "    num_continuous = 10,                # number of continuous values\n",
    "    dim = 32,                           # dimension, paper set at 32\n",
    "    dim_out = 1,                        # binary prediction, but could be anything\n",
    "    depth = 6,                          # depth, paper recommended 6\n",
    "    heads = 8,                          # heads, paper recommends 8\n",
    "    attn_dropout = 0.1,                 # post-attention dropout\n",
    "    ff_dropout = 0.1,                   # feed forward dropout\n",
    "    mlp_hidden_mults = (4, 2),          # relative multiples of each hidden dimension of the last mlp to logits\n",
    "    mlp_act = nn.ReLU(),                # activation for final mlp, defaults to relu, but could be anything else (selu etc)\n",
    "    continuous_mean_std = cont_mean_std # (optional) - normalize the continuous values before layer norm\n",
    ")\n",
    "\n",
    "x_categ = torch.randint(0, 5, (1, 5))     # category values, from 0 - max number of categories, in the order as passed into the constructor above\n",
    "x_cont = torch.randn(1, 10)               # assume continuous values are already normalized individually\n",
    "\n",
    "pred = model(x_categ, x_cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d65e8dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.ones(32,1,dtype=torch.int32) + torch.arange(10)\n",
    "outp = model(inp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd1fd599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10, 32])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outp.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "92d55ab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
