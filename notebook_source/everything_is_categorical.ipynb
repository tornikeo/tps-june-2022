{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a0254079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda torch.float32\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "import pandas as pd\n",
    "\n",
    "import gc\n",
    "import torch as th\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import transformers\n",
    "import torch.optim as optim\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "dtype = torch.bfloat32 if torch.cuda.is_bf16_supported() else \\\n",
    "  torch.float32\n",
    "print(device, dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "387d1c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "  '../input/tabular-playground-series-jun-2022/data.csv',\n",
    "  index_col=0,\n",
    ")\n",
    "sample_sub = pd.read_csv(\n",
    "  '../input/tabular-playground-series-jun-2022/sample_submission.csv',\n",
    "  index_col=0\n",
    ")\n",
    "# for col in df:\n",
    "#   if df[col].dtype == 'int64':\n",
    "#     df[col] = df[col].astype('int32')\n",
    "#   if df[col].dtype == 'float64':\n",
    "#     df[col] = df[col].astype('float32')\n",
    "\n",
    "df = df.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68360194",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.round(2).values.ravel().astype('float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f7c4106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from typing import Tuple\n",
    "from torch import Tensor\n",
    "def get_data() -> Tuple[Tensor, LabelEncoder]:\n",
    "  df = pd.read_csv(\n",
    "    '../input/tabular-playground-series-jun-2022/data.csv',\n",
    "    index_col=0,\n",
    "  )\n",
    "  le = LabelEncoder()\n",
    "  xt = le.fit_transform(x).astype('int16')\n",
    "  raw_idx = xt.reshape(df.shape)\n",
    "  \n",
    "  return raw_ids, le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c97445f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_ids, float_encoder = get_data()\n",
    "# xi = le.inverse_transform(xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "28c1e5f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3901"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_index = float_encoder.transform([np.nan])[0]\n",
    "nan_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1f34eb0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3901"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_ids.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9e1a5acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(vocab_size = None) -> Tuple[nn.Module, optim.Optimizer]:\n",
    "  config = transformers.DistilBertConfig\\\n",
    "    .from_pretrained('hf-internal-testing/tiny-random-distilbert')\n",
    "  if vocab_size:\n",
    "    config.vocab_size = vocab_size\n",
    "  model = AutoModel.from_config(config)\n",
    "  optim = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=.5)\n",
    "  return model, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "15647fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer = get_model(raw_ids.max() + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7f2480a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51550/1807916251.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  aten_mask = torch.tensor((inp_ids != nan_index), dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  inp_ids = torch.tensor(raw_ids[:1000], dtype=torch.int32)\n",
    "  aten_mask = torch.tensor((inp_ids != nan_index), dtype=torch.uint8)\n",
    "  outp = model(\n",
    "    input_ids=inp_ids,\n",
    "    attention_mask=aten_mask,\n",
    "    output_attentions = True,\n",
    "    output_hidden_states = True,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3e100eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 80, 32])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outp.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "71f072ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 0,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 0,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aten_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0dfe690a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=nan_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4904c62a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e8f35b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # input_ids = torch.randint(0, 1000, (32, 40))\n",
    "# input_ids = torch.ones(32, 40).int()\n",
    "# attention_mask = torch.randint(-100,100, (32, 40))\n",
    "# outp= model(\n",
    "#   input_ids=input_ids,\n",
    "#   attention_mask=attention_mask,\n",
    "#   input_embeds=\n",
    "#   output_attentions = True,\n",
    "#   output_hidden_states = True,)\n",
    "# # [[k,v.shape,v.dtype] for k,v in outp.items()]\n",
    "# print('Last hiddne state shape', outp.last_hidden_state.shape)\n",
    "# print('Number of tfm layers + 1 for embed', len(outp.hidden_states)) # \n",
    "# print(outp.hidden_states[0].shape)\n",
    "# print(outp.hidden_states[1].shape)\n",
    "# print('Attention shape', outp.attentions[0].shape)\n",
    "# print(len(outp.attentions)) # Number of tfm layers\n",
    "# outp.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a601f2d",
   "metadata": {},
   "source": [
    "TODO: Issue in torch library - unique returns multiple instances of Na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "399c45c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(1124, 32, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 32)\n",
       "    (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (out_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=32, out_features=37, bias=True)\n",
       "          (lin2): Linear(in_features=37, out_features=32, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (1): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (out_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=32, out_features=37, bias=True)\n",
       "          (lin2): Linear(in_features=37, out_features=32, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (2): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (out_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=32, out_features=37, bias=True)\n",
       "          (lin2): Linear(in_features=37, out_features=32, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (3): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (out_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=32, out_features=37, bias=True)\n",
       "          (lin2): Linear(in_features=37, out_features=32, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (4): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (out_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=32, out_features=37, bias=True)\n",
       "          (lin2): Linear(in_features=37, out_features=32, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "198e2189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.5449e-01, -4.6411e-01,  2.3047e+00,  ...,  3.8086e+00,\n",
       "          1.2363e+00,  1.1816e+00],\n",
       "        [ 1.3809e+00, -4.9951e-01, -4.1846e-01,  ..., -4.2188e+00,\n",
       "         -2.7246e+00, -6.3782e-02],\n",
       "        [ 2.5610e-01, -1.0596e+00,  0.0000e+00,  ..., -2.1309e+00,\n",
       "          3.6621e+00, -1.3159e-01],\n",
       "        ...,\n",
       "        [ 1.4758e-01, -7.1533e-01, -4.6509e-01,  ..., -5.8398e-01,\n",
       "         -1.4922e+00, -9.9756e-01],\n",
       "        [-1.7100e+00, -8.1396e-01, -1.8662e+00,  ..., -1.0859e+00,\n",
       "          3.1230e+00,  4.8294e-03],\n",
       "        [-8.0615e-01, -2.5253e-02, -8.7549e-01,  ...,  2.3340e+00,\n",
       "          5.4258e+00, -8.2861e-01]], dtype=torch.float16)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bbf8c2f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.5449e-01, -4.6411e-01,  2.3047e+00,  ...,  3.8086e+00,\n",
       "          1.2363e+00,  1.1816e+00],\n",
       "        [ 1.3809e+00, -4.9951e-01, -4.1846e-01,  ..., -4.2188e+00,\n",
       "         -2.7246e+00, -6.3782e-02],\n",
       "        [ 2.5610e-01, -1.0596e+00,  0.0000e+00,  ..., -2.1309e+00,\n",
       "          3.6621e+00, -1.3159e-01],\n",
       "        ...,\n",
       "        [ 1.4758e-01, -7.1533e-01, -4.6509e-01,  ..., -5.8398e-01,\n",
       "         -1.4922e+00, -9.9756e-01],\n",
       "        [-1.7100e+00, -8.1396e-01, -1.8662e+00,  ..., -1.0859e+00,\n",
       "          3.1230e+00,  4.8294e-03],\n",
       "        [-8.0615e-01, -2.5253e-02, -8.7549e-01,  ...,  2.3340e+00,\n",
       "          5.4258e+00, -8.2861e-01]], dtype=torch.float16)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a48947d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hf-internal-testing/tiny-random-distilbert were not used when initializing DistilBertModel: ['pre_classifier.weight', 'classifier.weight', 'vocab_layer_norm.weight', 'pre_classifier.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'qa_outputs.bias', 'vocab_layer_norm.bias', 'qa_outputs.weight', 'classifier.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d82d9f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
