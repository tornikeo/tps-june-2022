{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9860f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "dtype = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2945a5c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hf-internal-testing/tiny-random-distilbert were not used when initializing DistilBertModel: ['pre_classifier.weight', 'qa_outputs.bias', 'vocab_projector.bias', 'pre_classifier.bias', 'vocab_projector.weight', 'classifier.bias', 'classifier.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'qa_outputs.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tok = AutoTokenizer.from_pretrained('hf-internal-testing/tiny-random-distilbert')\n",
    "model = AutoModel.from_pretrained('hf-internal-testing/tiny-random-distilbert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979e5aec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0eb0c69b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(1124, 32, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 32)\n",
       "    (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (out_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=32, out_features=37, bias=True)\n",
       "          (lin2): Linear(in_features=37, out_features=32, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (1): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (out_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=32, out_features=37, bias=True)\n",
       "          (lin2): Linear(in_features=37, out_features=32, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (2): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (out_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=32, out_features=37, bias=True)\n",
       "          (lin2): Linear(in_features=37, out_features=32, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (3): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (out_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=32, out_features=37, bias=True)\n",
       "          (lin2): Linear(in_features=37, out_features=32, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (4): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (out_lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=32, out_features=37, bias=True)\n",
       "          (lin2): Linear(in_features=37, out_features=32, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "61c790cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/tornikeo/tab-transformer-pytorch\n",
      "  Cloning https://github.com/tornikeo/tab-transformer-pytorch to /tmp/pip-req-build-cq_qjc8g\n",
      "  Running command git clone -q https://github.com/tornikeo/tab-transformer-pytorch /tmp/pip-req-build-cq_qjc8g\n",
      "  Resolved https://github.com/tornikeo/tab-transformer-pytorch to commit 10cedc26ba54273fafe9ba2e15c47877a6d7b35f\n",
      "Collecting einops>=0.3\n",
      "  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: torch>=1.6 in /home/tornikeo/.miniconda3/envs/torch/lib/python3.8/site-packages (from tab-transformer-pytorch==0.1.4) (1.11.0)\n",
      "Requirement already satisfied: typing_extensions in /home/tornikeo/.miniconda3/envs/torch/lib/python3.8/site-packages (from torch>=1.6->tab-transformer-pytorch==0.1.4) (4.2.0)\n",
      "Building wheels for collected packages: tab-transformer-pytorch\n",
      "  Building wheel for tab-transformer-pytorch (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tab-transformer-pytorch: filename=tab_transformer_pytorch-0.1.4-py3-none-any.whl size=4579 sha256=6e59ce7fedd45a1c8f9ce22a63cbd71bc9d5b2391d713ec70278e0d01b589d81\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-bakx0yli/wheels/ea/d3/c1/b0003b87658c098a368fd6a003355be89412cbd2629d3985ec\n",
      "Successfully built tab-transformer-pytorch\n",
      "Installing collected packages: einops, tab-transformer-pytorch\n",
      "Successfully installed einops-0.4.1 tab-transformer-pytorch-0.1.4\n"
     ]
    }
   ],
   "source": [
    "# !pip install git+https://github.com/tornikeo/tab-transformer-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e0314528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tab_transformer_pytorch import TabTransformer\n",
    "\n",
    "cont_mean_std = torch.randn(10, 2)\n",
    "\n",
    "model = TabTransformer(\n",
    "    categories = (10, 5, 6, 5, 8),      # tuple containing the number of unique values within each category\n",
    "    num_continuous = 10,                # number of continuous values\n",
    "    dim = 32,                           # dimension, paper set at 32\n",
    "    dim_out = 1,                        # binary prediction, but could be anything\n",
    "    depth = 6,                          # depth, paper recommended 6\n",
    "    heads = 8,                          # heads, paper recommends 8\n",
    "    attn_dropout = 0.1,                 # post-attention dropout\n",
    "    ff_dropout = 0.1,                   # feed forward dropout\n",
    "    mlp_hidden_mults = (4, 2),          # relative multiples of each hidden dimension of the last mlp to logits\n",
    "    mlp_act = nn.ReLU(),                # activation for final mlp, defaults to relu, but could be anything else (selu etc)\n",
    "    continuous_mean_std = cont_mean_std # (optional) - normalize the continuous values before layer norm\n",
    ")\n",
    "\n",
    "x_categ = torch.randint(0, 5, (1, 5))     # category values, from 0 - max number of categories, in the order as passed into the constructor above\n",
    "x_cont = torch.randn(1, 10)               # assume continuous values are already normalized individually\n",
    "\n",
    "pred = model(x_categ, x_cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d65e8dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.ones(32,1,dtype=torch.int32) + torch.arange(10)\n",
    "outp = model(inp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd1fd599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10, 32])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outp.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "92d55ab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
