{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69c6866d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda torch.float32\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "import pandas as pd\n",
    "\n",
    "import gc\n",
    "import torch as th\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import transformers\n",
    "import transformers as tfms\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple\n",
    "from datasets import load_dataset\n",
    "import os \n",
    "\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "torch.manual_seed(0)\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "try:\n",
    "  FLOAT = torch.bfloat32 if torch.cuda.is_bf16_supported() else \\\n",
    "    torch.float32\n",
    "except Exception as e:\n",
    "  print(e)\n",
    "  FLOAT = torch.float32\n",
    "print(DEVICE, FLOAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11f034b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hf-internal-testing/tiny-random-distilbert were not used when initializing DistilBertForMaskedLM: ['qa_outputs.bias', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight', 'qa_outputs.weight', 'classifier.weight']\n",
      "- This IS expected if you are initializing DistilBertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# config = tfms.DistilBertConfig.from_pretrained('hf-internal-testing/tiny-random-distilbert')\n",
    "# model = tfms.AutoModelForMaskedLM.from_config(config)\n",
    "model = tfms.AutoModelForMaskedLM.from_pretrained('hf-internal-testing/tiny-random-distilbert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a1736f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset eli5 (/home/tornikeo/.cache/huggingface/datasets/eli5/LFQA_reddit/1.0.0/17574e5502a10f41bbd17beba83e22475b499fa62caa1384a3d093fc856fe6fa)\n"
     ]
    }
   ],
   "source": [
    "eli5 = load_dataset(\"eli5\", split=\"train_asks[:5000]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d57a26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5 = eli5.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3f6caa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q_id': '26rez4',\n",
       " 'title': 'Do white/red dwarfs have a Habitable Zone?',\n",
       " 'selftext': 'If a planet for all intents and purposes identical to Earth were to orbit around a white or red dwarf, would the \"dead\" star be able to supply enough energy to sustain life?\\n\\n\\nIf they can, where would the zone be, and how long could the star provide for it\\'s hypothetical life-bearing planet?',\n",
       " 'document': '',\n",
       " 'subreddit': 'askscience',\n",
       " 'answers': {'a_id': ['chu6qmj'],\n",
       "  'text': ['First off, a white dwarf is a \"dead\" star, while a red dwarf is just a small normal star. \\n\\nWhat we call a habitable zone is just the zone in which water on the surface would be liquid on a planet with sufficient atmospheric pressure. Because these dwarfs give off less energy than the sun, the zone is closer to the star.\\n\\nRed dwarfs would be able to provide life longer than the sun could, because red dwarfs live longer.\\n\\nWhite dwarfs most likely won\\'t support life because they cool down from when they form, which means that the zone keeps moving inward. So planets will only have a limited amount of time in the habitable zone'],\n",
       "  'score': [3]},\n",
       " 'title_urls': {'url': []},\n",
       " 'selftext_urls': {'url': []},\n",
       " 'answers_urls': {'url': []}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bbda04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tfms.AutoTokenizer.from_pretrained('hf-internal-testing/tiny-random-distilbert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffce2613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q_id': '26rez4',\n",
       " 'title': 'Do white/red dwarfs have a Habitable Zone?',\n",
       " 'selftext': 'If a planet for all intents and purposes identical to Earth were to orbit around a white or red dwarf, would the \"dead\" star be able to supply enough energy to sustain life?\\n\\n\\nIf they can, where would the zone be, and how long could the star provide for it\\'s hypothetical life-bearing planet?',\n",
       " 'document': '',\n",
       " 'subreddit': 'askscience',\n",
       " 'answers.a_id': ['chu6qmj'],\n",
       " 'answers.text': ['First off, a white dwarf is a \"dead\" star, while a red dwarf is just a small normal star. \\n\\nWhat we call a habitable zone is just the zone in which water on the surface would be liquid on a planet with sufficient atmospheric pressure. Because these dwarfs give off less energy than the sun, the zone is closer to the star.\\n\\nRed dwarfs would be able to provide life longer than the sun could, because red dwarfs live longer.\\n\\nWhite dwarfs most likely won\\'t support life because they cool down from when they form, which means that the zone keeps moving inward. So planets will only have a limited amount of time in the habitable zone'],\n",
       " 'answers.score': [3],\n",
       " 'title_urls.url': [],\n",
       " 'selftext_urls.url': [],\n",
       " 'answers_urls.url': []}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5 = eli5.flatten()\n",
    "eli5['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cbdcf27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['q_id',\n",
       " 'title',\n",
       " 'selftext',\n",
       " 'document',\n",
       " 'subreddit',\n",
       " 'answers.a_id',\n",
       " 'answers.text',\n",
       " 'answers.score',\n",
       " 'title_urls.url',\n",
       " 'selftext_urls.url',\n",
       " 'answers_urls.url']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfaa8f52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': ['q_id',\n",
       "  'title',\n",
       "  'selftext',\n",
       "  'document',\n",
       "  'subreddit',\n",
       "  'answers.a_id',\n",
       "  'answers.text',\n",
       "  'answers.score',\n",
       "  'title_urls.url',\n",
       "  'selftext_urls.url',\n",
       "  'answers_urls.url'],\n",
       " 'test': ['q_id',\n",
       "  'title',\n",
       "  'selftext',\n",
       "  'document',\n",
       "  'subreddit',\n",
       "  'answers.a_id',\n",
       "  'answers.text',\n",
       "  'answers.score',\n",
       "  'title_urls.url',\n",
       "  'selftext_urls.url',\n",
       "  'answers_urls.url']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "450c60b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function preprocess_function at 0x7f7d331b0820> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d99934bb1628431b9810946c01c6bb5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': (1000, 512, 'to', 512), 'attention_mask': (1000, 512, 'to', 512)}\n",
      "{'input_ids': (1000, 512, 'to', 512), 'attention_mask': (1000, 512, 'to', 512)}\n",
      "{'input_ids': (1000, 512, 'to', 512), 'attention_mask': (1000, 512, 'to', 512)}\n",
      "{'input_ids': (1000, 512, 'to', 512), 'attention_mask': (1000, 512, 'to', 512)}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e3e6b368f724d91bfb6a6036d2c787d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': (1000, 512, 'to', 512), 'attention_mask': (1000, 512, 'to', 512)}\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "  x = tokenizer([' '.join(ex) for ex\n",
    "                   in examples['answers.text']],\n",
    "                   truncation=True,\n",
    "                   padding=True,\n",
    "#                    return_tensors='np'\n",
    "                  )\n",
    "  print({k:(len(v),len(v[0]), 'to', len(v[-1])) for k,v in x.items()})\n",
    "  return x\n",
    "\n",
    "tokenized_eli5 = eli5.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=eli5[\"train\"].column_names,\n",
    "#     num_proc=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a06952e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 4000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8b416c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1dc28796f6d4873b86d1a4549f7cdd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7708b3ad164142e299de66f906bd0254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "block_size=128\n",
    "def group_texts(examples):\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "  \n",
    "\n",
    "lm_dataset = tokenized_eli5.map(group_texts, \n",
    "                                batched=True,) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83cc6ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# block_size = 128\n",
    "\n",
    "# z = None\n",
    "# def group_texts(examples):\n",
    "# #   global z\n",
    "#   #     print({k:(len(v),len(v[0])) for k,v in examples.items()})\n",
    "#   #     print({k:v[0][-1] for k,v in examples.items()})\n",
    "# #   try:\n",
    "# #     concatenated_examples = {k: sum(examples[k], []) \n",
    "# #                              for k in examples.keys()}\n",
    "#     print(list(examples.keys())[:4])\n",
    "#     concatenated_examples = {k: sum(examples[k], []) \n",
    "#                              for k in examples.keys()}\n",
    "#     #     print({k:(len(v)) for k,v in concatenated_examples.items()})\n",
    "#     total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "#     print({k:(len(v)) for k,v in concatenated_examples.items()})\n",
    "\n",
    "#     result = {\n",
    "#         k: [t[i : i + block_size] \n",
    "#             for i in range(0, total_length, block_size)]\n",
    "#         for k, t in concatenated_examples.items()\n",
    "#     }\n",
    "#     result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "#     return result\n",
    "\n",
    "# #   except TypeError as e:\n",
    "# #     z = examples\n",
    "# # #     raise e\n",
    "# #     print(e)\n",
    "# #     return None\n",
    "# lm_dataset = tokenized_eli5.map(group_texts, \n",
    "#                                 batched=True,) \n",
    "# #                                 num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c829c7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = tfms.AutoTokenizer.from_pretrained('distilbert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03326ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unk_token': '[UNK]',\n",
       " 'sep_token': '[SEP]',\n",
       " 'pad_token': '[PAD]',\n",
       " 'cls_token': '[CLS]',\n",
       " 'mask_token': '[MASK]'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9472dfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.pad_token = '[PAD]'\n",
    "data_collator = tfms.DataCollatorForLanguageModeling(\n",
    "  tokenizer=tokenizer,\n",
    "  mlm_probability=.15,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59d59585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 128)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(lm_dataset['test'][:5]['input_ids']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b56b4add",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms.TrainingArguments??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b0e3e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms.Trainer??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4e961400",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/hf-internal-testing/tiny-random-distilbert/resolve/main/config.json from cache at /home/tornikeo/.cache/huggingface/transformers/825fc05fc9603996d64ce61190f3b2eadb28e7a2f4db70d2cb44c8b5457deab6.5cd95c0833b01050f80cc06f242975c6b324790c205343941ec863daed8f33c8\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"hf-internal-testing/tiny-random-distilbert\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 32,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dim\": 37,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 4,\n",
      "  \"n_layers\": 5,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"vocab_size\": 1124\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/hf-internal-testing/tiny-random-distilbert/resolve/main/pytorch_model.bin from cache at /home/tornikeo/.cache/huggingface/transformers/299e0fe6b464b753ce289d6c1fb9824a02df0f1470e41e22f869eb931ae37e50.5e3b0095ecec242fbedad51a189b0675b0a5b3bebc1bb3cb0f784debb35526e7\n",
      "Some weights of the model checkpoint at hf-internal-testing/tiny-random-distilbert were not used when initializing DistilBertForMaskedLM: ['qa_outputs.bias', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight', 'qa_outputs.weight', 'classifier.weight']\n",
      "- This IS expected if you are initializing DistilBertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of DistilBertForMaskedLM were initialized from the model checkpoint at hf-internal-testing/tiny-random-distilbert.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = tfms.AutoModelForMaskedLM.from_pretrained('hf-internal-testing/tiny-random-distilbert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "faff718c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "!rm -rf results/*\n",
    "training_args = tfms.TrainingArguments(\n",
    "  output_dir='./results',\n",
    "#   evaluation_strategy=\"epoch\",\n",
    "  evaluation_strategy=\"steps\",\n",
    "  eval_steps=100,\n",
    "  learning_rate=2e-9,\n",
    "  num_train_epochs=3,\n",
    "  weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = tfms.Trainer(\n",
    "  model=model,\n",
    "  args=training_args,\n",
    "  train_dataset=lm_dataset['train'],\n",
    "  eval_dataset=lm_dataset['test'],\n",
    "  data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d804842f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskedLMOutput(loss=tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
       "       grad_fn=<AddBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = lm_dataset['train'][:5]\n",
    "x = {k:torch.tensor(v, device='cuda') for k,v in x.items()}\n",
    "y = model(**x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "682d9851",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 16000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='167' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 167/6000 00:06 < 03:36, 26.97 it/s, Epoch 0.08/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [48]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.miniconda3/envs/torch/lib/python3.8/site-packages/transformers/trainer.py:1409\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1406\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1407\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1408\u001b[0m )\n\u001b[0;32m-> 1409\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1410\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1414\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.miniconda3/envs/torch/lib/python3.8/site-packages/transformers/trainer.py:1718\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1716\u001b[0m     optimizer_was_run \u001b[38;5;241m=\u001b[39m scale_before \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m scale_after\n\u001b[1;32m   1717\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1718\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1720\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimizer_was_run \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed:\n\u001b[1;32m   1721\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.miniconda3/envs/torch/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:65\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     64\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.miniconda3/envs/torch/lib/python3.8/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.miniconda3/envs/torch/lib/python3.8/site-packages/transformers/optimization.py:370\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    367\u001b[0m     bias_correction2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m beta2 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    368\u001b[0m     step_size \u001b[38;5;241m=\u001b[39m step_size \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(bias_correction2) \u001b[38;5;241m/\u001b[39m bias_correction1\n\u001b[0;32m--> 370\u001b[0m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;66;03m# Just adding the square of the weights to the loss function is *not*\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;66;03m# the correct way of using L2 regularization/weight decay with Adam,\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;66;03m# since that will interact with the m and v parameters in strange ways.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;66;03m# of the weights to the loss with plain (non-momentum) SGD.\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;66;03m# Add weight decay at the end (fixed version)\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0100e87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
